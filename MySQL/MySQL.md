# MySQL

------

## 1、MySQL基础架构

![image-20210901143311584](\img\1)

MySQL的**缓存查询**功能在8.0后被删掉了。

------

## 2、redo log（重做日志）和 binlog（归档日志）

### redo log，以数据页为单位

redo log 是 InnoDB 引擎特有的日志， 是物理日志，记录的是“**在某个数据页上做了什么修改**”；

redo log 是循环写的，**空间固定会用完**

**WAL** 的全称是 Write-Ahead Logging，它的关键点就是**先写日志，再写磁盘**

![image-20210901151206387](img\2)

有了 **redo log**，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**

### redo log 的写入机制

事务在执行过程中，生成的 **redo log** 是要先写到 **redo log buffer** 的。

如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。

真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。

![image-20210904144446808](\img\8)

InnoDB 有一个后台线程，**每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache**，然后调用 fsync 持久化到磁盘。

事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，**一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的**。

**并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。**



### 重要的日志模块：binlog

Server 层也有自己的日志，称为 **binlog**（归档日志）

binlog 是逻辑日志，**记录的是这个语句的原始逻辑**

binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的**日志**。

**整个流程**

1. 执行器先找引擎取 **ID=2 这一行**。ID 是主键，引擎直接用**树搜索**找到这一行。如果 ID=2 这一行所在的**数据页**本来就在内存中，就直接返回给**执行器**；否则，需要**先从磁盘读入内存**，然后**再返回**。
2. 执行器拿到引擎给的**行数据**，把这个值加上 1，比如原来是 N，现在就是 **N+1**，得到**新的一行**数据，再调用引擎接口写入这行新数据。
3. 引擎将这行新数据**更新到内存**中，同时将这个更新操作**记录到 redo log** 里面，此时 redo log 处于 **prepare** 状态。然后**告知执行器**执行完成了，随时可以提交事务。
4. 执行器生成这个操作的 **binlog**，并把 **binlog** 写入磁盘。
5. 执行器调用引擎的**提交事务接口**，引擎把刚刚写入的 **redo log 改成提交（commit）状态**，更新完成。

![image-20210901155043587](\img\3)

### 两阶段提交

组提交（group commit）机制

### binlog 的写入机制

其实，binlog 的写入逻辑比较简单：事务执行过程中，先把日志写到 **binlog cache**，事务提交的时候，再把 **binlog cache 写到 binlog 文件中**。

一个事务的 binlog 是不能被拆开的，因此**不论这个事务多大，也要确保一次性写入**。这就涉及到了 binlog cache 的保存问题。

系统给 **binlog cache** 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。

事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 **binlog cache**。

![image-20210904144109596](\img\7)

每个线程有自己 binlog cache，但是共用同一份 binlog 文件。

- 图中的 **write**，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。
- 图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 **fsync 才占磁盘的 IOPS**。

### Binlog主从一致

一个 update 语句在节点 A 执行，然后同步到节点 B 的完整流程图。

![image-20210904152457351](\img\9)

备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的：

1. 在备库 B 上通过 **change master** 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。
2. 在备库 B 上执行 **start slave** 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。
3. 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，**从本地读取 binlog**，发给 B。
4. 备库 B 拿到 binlog 后，写到本地文件，称为**中转日志（relay log）**。
5. sql_thread 读取中转日志，解析出日志里的命令，并执行。

### binlog 有三种格式

一种是 **statement，一种是 row，一种是 mixed**

**由于 statement 格式下**，记录到 binlog 里的**是语句原文**，因此可能会出现这样一种情况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 t_modified。因此，MySQL 认为这样写是有风险的。

**row 格式的时候**，binlog 里面记录了真实删除行的主键 id，这样 binlog 传到备库去的时候，就肯定会删除 id=4 的行，不会有主备删除不同行的问题。

**row 格式的缺点**是，**很占空间**。比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。这样做，不仅会占用更大的空间，同时写 binlog 也要耗费 IO 资源，影响执行速度。

**mixed 格式**、MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。

 **row格式的好处：恢复数据** delete 语句，row 格式的 binlog 也会把被删掉的行的整行信息保存起来。



------

## 3、隔离性与隔离级别

在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。

在**“可重复读”**隔离级别下，这个视图是在**事务启动时创建**的，整个**事务存在期间都用这个视图**。

在“**读提交**”隔离级别下，这个视图是在每**个 SQL 语句开始执行的时候创建的**。

这里需要注意的是，“**读未提交**”隔离级别下**直接返回记录上的最新值，没有视图概念**；

而“串行化”隔离级别下**直接用加锁的方式来避免并行访问**。

### 事务隔离的实现

**不同时刻启动的事务会有不同的 read-view**。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是**数据库的多版本并发控制（MVCC）**。

回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当**没有事务再需要用到这些回滚日志**时，**回滚日志会被删除**。

当系统里没有比这个回滚日志更早的 read-view 的时候

**尽量不要使用长事务**。

长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。

### 事务的启动方式

1. 显式启动事务语句， **begin 或 start transaction**。配套的提交语句是 **commit**，回滚语句是 **rollback**。
2. **set autocommit=0**，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。

有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

因此，我会建议你总是使用 **set autocommit=1**, 通过显式语句的方式来启动事务。

------

## 4、索引

根据叶子节点的内容，索引类型分为**主键索引和非主键索引**

主键索引的叶子节点存的是整行数据。在 InnoDB 里，**主键索引**也被称为**聚簇索引**（clustered index）。

非主键索引的叶子节点内容是主键的值。在 InnoDB 里，**非主键索引**也被称为**二级索引**（secondary index）。

基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用**主键查询**。

### 索引的失效情况

> 1.有or必全有索引;
> 2.复合索引未用左列字段;
> 3.like以%开头;
> 4.需要类型转换;
> 5.where中索引列有运算;
> 6.where中索引列使用了函数;
> 7.如果mysql觉得全表扫描更快时（数据少）;

### 覆盖索引

select ID from T where k between 3 and 5，这时只需要查 ID 的值，而 **ID 的值已经在 k 索引树上**了，因此可以直接提供查询结果，**不需要回表**。

据市民的身份证号查询他的姓名，这个联合索引就有意义了。它可以在这个**高频**请求上用到覆盖索引，不再需要回表查整行记录

### 最左前缀原则

因此，**第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。**

![image-20210903230540529](\img\6)

### 索引下推

MySQL 5.6 引入的**索引下推优化**（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接**过滤掉不满足条件的记录**，减少回表次数。

![image-20210901172841186](\img\4)

### 更新过程

当需要**更新一个数据页**时，如果**数据页在内存中就直接更新**，而如果这个数据页还**没有在内存中的话**，在不影响数据一致性的前提下，**InooDB 会将这些更新操作缓存在 change buffer 中**，这样就不需要从磁盘中读入这个数据页了。在下次查询**需要访问这个数据页**的时候，将数据页读入内存，然后执行 **change buffer 中与这个页有关的操作**。通过这种方式就能保证这个数据逻辑的正确性。

**change buffer，实际上它是可以持久化的数据**

**唯一索引的更新就不能使用 change buffer**，实际上**也只有普通索引**可以使用



**如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。**

第一种情况是，**这个记录要更新的目标页在内存中**。这时，InnoDB 的处理流程如下：

- 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。

这样看来，**普通索引和唯一索引**对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。

但，这不是我们关注的重点。

第二种情况是，**这个记录要更新的目标页不在内存中**。这时，InnoDB 的处理流程如下：

- 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，则是将更新记录在 **change buffer**，语句执行就结束了。

将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。**change buffer 因为减少了随机磁盘**访问，所以对更新性能的提升是会很明显的。

### 索引选择和实践

回到我们文章开头的问题，普通索引和唯一索引应该怎么选择。其实，这两类索引在**查询能力上是没差别**的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择**普通索引**。

如果所有的**更新后面，都马上伴随着对这个记录的查询**，那么你应该关闭 **change buffer**。而在其他情况下，**change buffer 都能提升更新性能**。

**普通索引和 change buffer** 的配合使用，对于数据量大的表的更新优化还是很明显的。



**慢查询日志概念**

   MySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阀值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。

使用 **force index(a)** 来让优化器强制使用索引 **a**



**索引的建立**

比如，这两个在 email 字段上创建索引的语句：

```
mysql> alter table SUser add index index1(email);

或

mysql> alter table SUser add index index2(email(6));
```

第一个语句创建的 index1 索引里面，包含了每个记录的整个字符串；而第二个语句创建的 index2 索引里面，对于每个记录都是只取前 6 个字节。

占用的空间会更小，这就是使用**前缀索引**的优势。

**使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**

使用前缀索引可能会增加扫描行数，这会影响到性能。



储存身份证的情况，可以倒叙，可以利用hash

------

## 5、全局锁表级锁

MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

**全局锁的典型使用场景是，做全库逻辑备份。**也就是把整库每个表都 select 出来存成文本。

### 表级锁

MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁

**表锁的语法是 lock tables … read/write。**与 FTWRL 类似，可以用 unlock tables 主动释放锁

在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。

### 行锁

**在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。**

如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。

### 死锁和死锁检测

**有两种策略：**

- 一种策略是，直接进入等待，直到超时。这个**超时时间**可以通过参数 innodb_lock_wait_timeout 来设置。
- 另一种策略是，发起死锁检测，发现死锁后，**主动回滚死锁链条中的某一个事务**，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。

在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。

主动死锁检测，而且 innodb_deadlock_detect 的默认值本身就是 on。主动死锁检测在发生死锁的时候，是能**够快速发现并进行处理的，**但是它也是有额外负担的。死锁检测要耗费大量的 CPU 资源。

将一行改成逻辑上的多行来减少锁冲突

## 6、事务的隔离

begin/start transaction 命令并不是一个事务的起点，在**执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动**。如果你想要马上启动一个事务，可以使用 **start transaction with consistent snapshot** 这个命令。

**第一种启动方式**，一致性视图是在第执行第一个快照读语句时创建的；

**第二种启动方式**，一致性视图是在执行 **start transaction with consistent snapshot** 时创建的。

在 MySQL 里，有两个“视图”的概念：

- 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。
- 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 **RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读**）隔离级别的实现。

### “快照”在 MVCC 里是怎么工作的？

可重复读隔离级别下，事务在**启动的时候就“拍了个快照”**。注意，这个快照是基于整库的。

每个**事务有一个唯一的事务 ID**，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。

每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。

数据表中的一行记录，其实可能有**多个版本** (row)，每个版本有自己的 row trx_id。

######  **undo log**

语句更新会生成 **undo log**（**回滚日志**）吗？那么，**undo log 在哪呢？**

![image-20210901182931142](\img\5)

而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。

**InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。**

### 更新逻辑

**更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）**

```sql
mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;
```

下面这两个 select 语句，就是分别加了读锁（S 锁，共享锁）和写锁（X 锁，排他锁）。

**事务的可重复读的能力是怎么实现的？**

可**重复读的核心就是一致性读**（consistent read）；而**事务更新数据的时候，只能用当前读**。如果当前的记录的**行锁被其他事务占用的话，就需要进入锁等待**。

- 在**可重复读**隔离级别下，只需要在**事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图**；
- 在读**提交隔离级别下，每一个语句执行前都会重新算出一个新的视图**。

## 7、重建表可以解决空间收缩

而在**MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。**

我给你简单描述一下引入了 Online DDL 之后，重建表的流程：

1. 建立一个临时文件，扫描表 A 主键的所有数据页；
2. 用数据页中表 A 的记录生成 B+ 树，存储到临时文件中；
3. 生成临时文件的过程中，将所有对 A 的操作记录在一个日志文件（row log）中，对应的是图中 state2 的状态；
4. 临时文件生成后，将日志文件中的操作应用到临时文件，得到一个逻辑数据上与表 A 相同的数据文件，对应的就是图中 state3 的状态；
5. 用临时文件替换表 A 的数据文件。

## 8、不同的 count 用法

count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。

**对于 count(主键 id) 来说**，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。

**对于 count(1) 来说**，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。

单看这两个用法的差别的话，你能对比出来，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。

**对于 count(字段) 来说**：

1. 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加；
2. 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。

**但是 count(\*) 是例外**，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。

**count(字段) < count(主键 id) < count(1) ≈ count(\*)**，所以我建议你，尽量使用 count(\*)

------

## 9、如何解决幻读？

只有**RR下**的**当前读**才会导致幻读，需要间隙锁来解决



------

## 10、主从切换

### 循环复制问题

![image-20210904154025009](\img\10)

节点 A 同时是节点 B 的备库，相当于又把节点 B 新生成的 binlog 拿过来执行了一次，然后节点 A 和 B 间，会不断地循环执行这个更新语句，也就是循环复制了

可以用下面的逻辑，来解决两个节点间的循环复制的问题：

1. 规定两个库的 server id 必须不同，如果相同，则它们之间不能设定为主备关系；
2. 一个备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog；
3. 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。

### MySQL 的高可用

#### 主备延迟

主备延迟最直接的表现是，备库消费**中转日志**（relay log）的速度，比主库生产 binlog 的速度要慢。

#### 主备延迟的来源

**备库的压力大**

**即大事务**

一次性地用 delete 语句删除太多数据

另一种典型的大事务场景，就是大表 DDL

备库的并行复制能力

**由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略。**

#### 可靠性优先策略

1. 判断备库 B 现在的 seconds_behind_master，如果小于某个值（比如 5 秒）继续下一步，否则持续重试这一步；
2. 把主库 A 改成只读状态，即把 readonly 设置为 true；
3. 判断备库 B 的 seconds_behind_master 的值，直到这个值变成 0 为止；
4. 把备库 B 改成可读写状态，也就是把 readonly 设置为 false；
5. 把业务请求切到备库 B。

![image-20210904162121467](\img\11)

#### 可用性优先策略

把步骤 4、5 调整到最开始执行，也就是说不等主备数据同步，直接把连接切到备库 B，并且让备库 B 可以读写，那么系统几乎就没有不可用时间了。

可能出现数据不一致的情况





------

# SQL部分(SQL必知必会)

1、DDL，英文叫做 Data Definition Language，也就是数据定义语言

2、DML，英文叫做 Data Manipulation Language，数据操作语言

3、DCL，英文叫做 Data Control Language，数据控制语言

4、DQL，英文叫做 Data Query Language，数据查询语言

文档型数据库MongoDB

**搜索引擎的优势在于采用了全文搜索技术“倒排索引”。**

列式数据库是将数据按照列存储到数据库中，这样的好处是大量降低系统I/O，适合分布式文件

图形数据库，利用了实体对象之间的关系。最典型就是人与人的关系

![image-20210903152732582](img\sql1)

## DDL 的基础语法及设计工具

DDL 的英文全称是 Data Definition Language，中文是数据定义语言。它定义了数据库的结构和数据表的结构。

在 DDL 中，我们常用的功能是增删改，分别对应的命令是 CREATE、DROP 和 ALTER。需要注意的是，在执行 DDL 的时候，不需要 COMMIT，就可以完成执行任务。

### 外键约束。

外键确保了表与表之间引用的完整性。一个表中的外键对应另一张表的主键。外键可以是重复的，也可以为空。比如 player_id 在 player 表中是主键，如果你想设置一个球员比分表即 player_score，就可以在 player_score 中设置 player_id 为外键，关联到 player 表中。

### 设计数据表的原则

“**三少一多**”原则：

1.**数据表的个数越少越好**

RDBMS 的核心在于对实体和联系的定义，也就是 E-R 图（Entity Relationship Diagram），数据表越少，证明实体和联系设计得越简洁，既方便理解又方便操作。

2.**数据表中的字段个数越少越好**

字段个数越多，数据冗余的可能性越大。设置字段个数少的前提是各个字段相互独立，而不是某个字段的取值可以由其他字段计算出来。当然字段个数少是相对的，我们通常会在数据冗余和检索效率中进行平衡。

3.**数据表中联合主键的字段个数越少越好**

设置主键是为了确定唯一性，当一个字段无法确定唯一性的时候，就需要采用联合主键的方式（也就是用多个字段来定义一个主键）。联合主键中的字段越多，占用的索引空间越大，不仅会加大理解难度，还会增加运行时间和索引空间，因此联合主键的字段个数越少越好。

4.**使用主键和外键越多越好**

数据库的设计实际上就是定义各种表，以及各种字段之间的关系。这些关系越多，证明这些实体之间的冗余度越低，利用度越高。这样做的好处在于不仅保证了数据表之间的独立性，还能提升相互之间的关联使用率。

你应该能看出来“三少一多”原则的核心就是简单可复用。简单指的是用更少的表、更少的字段、更少的联合主键字段来完成数据表的设计。可复用则是通过主键、外键的使用来增强数据表之间的复用率。因为一个主键可以理解是一张表的代表。键设计得越多，证明它们之间的利用率越高。

### 检索数据SELECT

关于单个表的 SELECT 查询，还有一个非常实用的操作，就是从结果中去掉重复的行。使用的关键字是 DISTINCT。比如我们想要看下 heros 表中关于攻击范围的取值都有哪些：

```
SQL：SELECT DISTINCT attack_range FROM heros
```

1. **DISTINCT** 需要放到所有列名的前面，如果写成`SELECT name, DISTINCT attack_range FROM heros`会报错。
2. **DISTINCT** 其实是对后面所有列名的组合进行去重，你能看到最后的结果是 69 条，因为这 69 个英雄名称不同，都有攻击范围（attack_range）这个属性值。如果你想要看都有哪些不同的攻击范围（attack_range），只需要写`DISTINCT attack_range`即可，后面不需要再加其他的列名了。

### 排序检索数据

**1、排序的列名：**ORDER BY 后面可以有一个或多个列名，如果是多个列名进行排序，会按照后面第一个列先进行排序，当第一列的值相同的时候，再按照第二列进行排序，以此类推。

**2、排序的顺序：**ORDER BY 后面可以注明排序规则，ASC 代表递增排序，DESC 代表递减排序。如果没有注明排序规则，默认情况下是按照 ASC 递增排序。

**3、非选择列排序：**ORDER BY 可以使用非选择列进行排序，所以即使在 SELECT 后面没有这个列名，你同样可以放到 ORDER BY 后面进行排序。

**4、ORDER BY 的位置**：ORDER BY 通常位于 SELECT 语句的最后一条子句，否则会报错。

### SELECT 的执行顺序

1. 关键字的顺序是不能颠倒的：

```
SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ...
```

2.SELECT 语句的执行顺序（在 MySQL 和 Oracle 中，SELECT 执行顺序基本相同）：

```
FROM > WHERE > GROUP BY > HAVING > SELECT 的字段 > DISTINCT > ORDER BY > LIMIT
```

  **SQL的执行原理**

首先，你可以注意到，SELECT 是先执行 FROM 这一步的。在这个阶段，如果是多张表联查，还会经历下面的几个步骤：

1. 首先先通过 CROSS JOIN 求笛卡尔积，相当于得到虚拟表 vt（virtual table）1-1；
2. 通过 ON 进行筛选，在虚拟表 vt1-1 的基础上进行筛选，得到虚拟表 vt1-2；
3. 添加外部行。如果我们使用的是左连接、右链接或者全连接，就会涉及到外部行，也就是在虚拟表 vt1-2 的基础上增加外部行，得到虚拟表 vt1-3。

> 当我们拿到了查询数据表的原始数据，也就是最终的虚拟表 vt1，就可以在此基础上再进行 WHERE 阶段。在这个阶段中，会根据 vt1 表的结果进行筛选过滤，得到虚拟表 vt2。
>
> 然后进入第三步和第四步，也就是 GROUP 和 HAVING 阶段。在这个阶段中，实际上是在虚拟表 vt2 的基础上进行分组和分组过滤，得到中间的虚拟表 vt3 和 vt4。
>
> 当我们完成了条件筛选部分之后，就可以筛选表中提取的字段，也就是进入到 SELECT 和 DISTINCT 阶段。
>
> 首先在 SELECT 阶段会提取想要的字段，然后在 DISTINCT 阶段过滤掉重复的行，分别得到中间的虚拟表 vt5-1 和 vt5-2。
>
> 当我们提取了想要的字段数据之后，就可以按照指定的字段进行排序，也就是 ORDER BY 阶段，得到虚拟表 vt6。
>
> 最后在 vt6 的基础上，取出指定行的记录，也就是 LIMIT 阶段，得到最终的结果，对应的是虚拟表 vt7。
>
> 当然我们在写 SELECT 语句的时候，不一定存在所有的关键字，相应的阶段就会省略。

### SQL数据过滤

**WHERE 子句中同时存在 OR 和 AND 的时候，AND 执行的优先级会更高，也就是说 SQL 会优先处理 AND 操作符，然后再处理 OR 操作符。**

### 通配符进行过滤

通配符就是我们用来匹配值的一部分的特殊字符。这里我们需要使用到 LIKE 操作符。

```
SQL：SELECT name FROM heros WHERE name LIKE '% 太 %'
```

（%）和（`_`）的区别在于，（%）代表一个或多个字符，而（`_`）只代表一个字符。

LIKE 后面就不能以（%）开头索引会失效

### SQL的聚集函数

SQL 中的聚集函数一共包括 5 个

![image-20210903173130769](\img\sql2)

COUNT(*) 只是统计数据行数，不管某个字段是否为 NULL

`COUNT(role_assist)`会忽略值为 NULL 的数据行

### 如何使用 HAVING 过滤分组

WHERE 是用于数据行，而 HAVING 则作用于**分组**

### EXISTS 子查询

### 集合比较子查询

### 将子查询作为计算字段

### SQL92 标准

SQL99，SQL92 规则更简单，更适合入门。

 5 种连接方式，它们分别是**笛卡尔积、等值连接、非等值连接、外连接（左连接、右连接）和自连接**。

### 笛卡尔积

 X 和 Y，那么 X 和 Y 的笛卡尔积就是 X 和 Y 的所有可能组合，也就是第一个对象来自于 X，第二个对象来自于 Y 的所有可能。

```sql
SQL: SELECT * FROM player, team
```

### 等值连接

```sql
SQL: SELECT player_id, player.team_id, player_name, height, team_name FROM player, team WHERE player.team_id = team.team_id
```

### 非等值连接

```sql
SQL：SELECT p.player_name, p.height, h.height_level
FROM player AS p, height_grades AS h
WHERE p.height BETWEEN h.height_lowest AND h.height_highest
```

### 外连接

### 自连接

### SQL99 标准中的连接查询

### 交叉连接

```sql
SQL: SELECT * FROM t1 CROSS JOIN t2 CROSS JOIN t3
```

### 自然连接

```sql
SELECT player_id, team_id, player_name, height, team_name FROM player NATURAL JOIN team 
```

### ON 连接

```sql
SELECT player_id, player.team_id, player_name, height, team_name FROM player JOIN team ON player.team_id = team.team_id
```

### USING 连接

```sql
SELECT player_id, team_id, player_name, height, team_name FROM player JOIN team USING(team_id)
```

------

## SQL 性能优化篇

### 第一步，选择适合的 DBMS

选择合适的数据库引擎

### 第二步，优化表设计

1. 表结构要尽量遵循第三范式的原则（关于第三范式，我在后面章节会讲）。这样可以让数据结构更加清晰规范，减少冗余字段，同时也减少了在更新，插入和删除数据时等异常情况的发生。
2. 如果分析查询应用比较多，尤其是需要进行多表联查的时候，可以采用反范式进行优化。反范式采用空间换时间的方式，通过增加冗余字段提高查询的效率。
3. 表字段的数据类型选择，关系到了查询效率的高低以及存储空间的大小。一般来说，**如果字段可以采用数值类型就不要采用字符类型**；**字符长度要尽可能设计得短一些**。针对字符类型来说，**当确定字符长度固定时，就可以采用 CHAR 类型**；**当长度不固定时，通常采用 VARCHAR 类型**。

### 第三步，优化逻辑查询

### 第四步，优化物理查询

### 第五步，使用 Redis 或 Memcached 作为缓存

除了可以对 SQL 本身进行优化以外，我们还可以请外援提升查询的效率。

### 第六步，库级优化

库级优化是站在数据库的维度上进行的优化策略，比如控制一个库中的数据表数量。另外我们可以采用主从架构优化我们的读写策略。

比如用主数据库（master）完成写操作，用从数据库（slave）完成读操作。

### 数据库的设计范式

目前关系型数据库一共有 6 种范式，按照范式级别，从低到高分别是：1NF（第一范式）、2NF（第二范式）、3NF（第三范式）、BCNF（巴斯 - 科德范式）、4NF（第四范式）和 5NF（第五范式，又叫做完美范式）。

![image-20210903183719223](\img\sql3)

### 数据表中的那些键

**1NF 指的是数据库表中的任何属性都是原子性的，不可再分**。

**2NF 指的数据表里的非主属性都要和这个数据表的候选键有完全依赖关系**。2NF 告诉我们一张表就是一个独立的对象，也就是说一张表只表达一个意思。

**3NF 在满足 2NF 的同时，对任何非主属性都不传递依赖于候选键**

球员编号决定了球队名称，同时球队名称决定了球队主教练，非主属性球队主教练就会传递依赖于球员编号，因此不符合 3NF 的要求。

### 反范式设计

#### BCNF（巴斯范式）

不存在主属性对于候选键的部分依赖或传递依赖

## 索引的概览

从功能逻辑上说，索引主要有 4 种，分别是**普通索引、唯一索引、主键索引和全文索引**。

**普通索引**是基础的索引，没有任何约束，主要用于提高查询效率。

**唯一索引**就是在普通索引的基础上增加了数据唯一性的约束，在一张数据表里可以有多个唯一索引。

**主键索引**在唯一索引的基础上增加了不为空的约束，也就是 NOT NULL+UNIQUE，一张表里最多只有一个主键索引。

**全文索引**用的不多，MySQL 自带的全文索引只支持英文。

按照物理实现方式，索引可以分为 2 种：**聚集索引和非聚集索引**。

**聚集索引**可以按照主键来排序存储数据

**聚集索引**指表中数据行按索引的排序方式进行存储，对查找行很有效。只有当表包含聚集索引时，表内的数据行才会按找索引列的值在磁盘上进行物理排序和存储。每一个表只能有一个聚集索引，因为数据行本身只能按一个顺序存储。

#### 联合索引的最左原则

## 什么是 B 树

平衡的多路搜索树

1. 所有叶子节点位于同一层。

### 什么是 B+ 树

1. 有 k 个孩子的节点就有 k 个关键字。也就是孩子数量 = 关键字数，而 B 树中，孩子数量 = 关键字数 +1。
2. 非叶子节点的关键字也会同时存在在子节点中，并且是在子节点中所有关键字的最大（或最小）。
3. 非叶子节点仅用于索引，不保存数据记录，跟记录有关的信息都放在叶子节点中。而 B 树中，非叶子节点既保存索引，也保存数据记录。
4. 所有关键字都在叶子节点出现，叶子节点构成一个有序链表，而且叶子节点本身按照关键字的大小从小到大顺序链接。

B+ 树的中间节点并不直接存储数据。这样的好处都有什么呢？

首先，B+ 树查询效率更稳定。因为 B+ 树每次只有访问到叶子节点才能找到对应的数据

B+ 树的查询效率更高，这是因为通常 B+ 树比 B 树更矮胖（阶数更大，深度更低），查询所需要的磁盘 I/O 也会更少。

## Hash 索引与 B+ 树索引的区别

1. Hash 索引不能进行范围查询，而 B+ 树可以。这是因为 Hash 索引指向的数据是无序的，而 B+ 树的叶子节点是个有序的链表。
2. Hash 索引不支持联合索引的最左侧原则（即联合索引的部分索引无法使用），而 B+ 树可以。对于联合索引来说，Hash 索引在计算 Hash 值的时候是将索引键合并后再一起计算 Hash 值，所以不会针对每个索引单独计算 Hash 值。因此如果用到联合索引的一个或者几个索引时，联合索引无法被利用。
3. Hash 索引不支持 ORDER BY 排序，因为 Hash 索引指向的数据是无序的，因此无法起到排序优化的作用，而 B+ 树索引数据是有序的，可以起到对该字段 ORDER BY 排序优化的作用。同理，我们也无法用 Hash 索引进行模糊查询，而 B+ 树使用 LIKE 进行模糊查询的时候，LIKE 后面前模糊查询（比如 % 开头）的话就可以起到优化作用。

## 数据库中的存储结构是怎样的

记录是按照行来存储的，但是数据库的读取并不以行为单位

**在数据库中，不论读一行，还是读多行，都是将这些行所在的页进行加载。也就是说，数据库管理存储空间的基本单位是页（Page）**

![image-20210904102952317](\img\sql4)

**区（Extent）**是比页大一级的存储结构，在 InnoDB 存储引擎中，一个区会分配 64 个连续的页。因为 InnoDB 中的页大小默认是 16KB，所以一个区的大小是 64*16KB=1MB。

**段（Segment）**由一个或多个区组成，区在文件系统是一个连续分配的空间（在 InnoDB 中是连续的 64 个页），不过在段中不要求区与区之间是相邻的。段是数据库中的分配单位，不同类型的数据库对象以不同的段形式存在。当我们创建数据表、索引的时候，就会相应创建对应的段，比如创建一张表时会创建一个表段，创建一个索引时会创建一个索引段。

**表空间（Tablespace）**是一个逻辑容器，表空间存储的对象是段，在一个表空间中可以有一个或多个段，但是一个段只能属于一个表空间。数据库由一个或多个表空间组成，表空间从管理上可以划分为系统表空间、用户表空间、撤销表空间、临时表空间等。

在 InnoDB 中存在两种表空间的类型：共享表空间和独立表空间。如果是共享表空间就意味着多张表共用一个表空间。如果是独立表空间，就意味着每张表有一个独立的表空间，也就是数据和索引信息都会保存在自己的表空间中。独立的表空间可以在不同的数据库之间进行迁移。

## 数据页内的结构是怎样的

页（Page）如果按类型划分的话，常见的有数据页（保存 B+ 树节点）、系统页、Undo 页和事务数据页等。数据页是我们最常使用的页。

表页的大小限定了表行的最大长度，不同 DBMS 的表页大小不同。

数据页包括七个部分，分别是文件头（File Header）、页头（Page Header）、最大最小记录（Infimum+supremum）、用户记录（User Records）、空闲空间（Free Space）、页目录（Page Directory）和文件尾（File Tailer）。

![image-20210904103609074](\img\sql5)

文件头中有两个字段，分别是 FIL_PAGE_PREV 和 FIL_PAGE_NEXT，它们的作用相当于指针，分别指向上一个数据页和下一个数据页。连接起来的页相当于一个双向的链表

文件尾的校验方式就是采用 Hash 算法进行校验。举个例子，当我们进行页传输的时候，如果突然断电了，造成了该页传输的不完整，这时通过文件尾的校验和（checksum 值）与文件头的校验和做比对，如果两个值不相等则证明页的传输有问题，需要重新进行传输，否则认为页的传输已经完成。

**B+ 树是如何进行记录检索的？**

从 B+ 树的根开始，逐层检索，直到找到叶子节点，也就是找到对应的数据页为止，将数据页加载到内存中，页目录中的槽（slot）采用二分查找的方式先找到一个粗略的记录分组，然后再在分组中通过链表遍历的方式查找记录。

## 数据库缓冲池

在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。

当我们对数据库中的记录进行修改的时候，首先会修改缓冲池中页里面的记录信息，然后数据库会以一定的频率刷新到磁盘上。注意并不是每次发生更新操作，都会立刻进行磁盘回写。缓冲池会采用一种叫做 checkpoint 的机制将数据回写到磁盘上，这样做的好处就是提升了数据库的整体性能。

需要释放掉一些不常用的页，就可以采用强行采用 checkpoint 的方式，将不常用的脏页回写到磁盘上，然后再从缓冲池中将这些页释放掉。

### 宽索引避免回表

我们可以通过宽索引将 SELECT 中需要用到的列（主键列可以除外）都设置在宽索引中，这样就避免了回表扫描的情况，从而提升 SQL 查询效率。

## SQL 查询的理想索引设计：三星索引

1. 在 WHERE 条件语句中，找到所有等值谓词中的条件列，将它们作为索引片中的开始列；
2. 将 GROUP BY 和 ORDER BY 中的列加入到索引中；
3. 将 SELECT 字段中剩余的列加入到索引片中。

采用三星索引会让索引片变宽

增加了索引维护的成本

## 按照锁粒度进行划分

锁定对象的粒度大小来对锁进行划分，分别为行锁、页锁和表锁。

## 数据库管理的角度对锁进行划分

共享锁和排它锁

共享锁也叫读锁或 S 锁，共享锁锁定的资源可以被其他用户读取，但不能修改。

```sql
LOCK TABLE product_comment READ;
```

排它锁也叫独占锁、写锁或 X 锁

排它锁锁定的数据只允许进行锁定操作的事务使用，其他事务无法对已锁定的数据进行查询或修改。

```sql
LOCK TABLE product_comment WRITE;
```

这时我们释放掉排它锁，使用这行命令即可。

```sql
UNLOCK TABLE;
```

### 意向锁（Intent Lock）

**给更大一级别的空间示意里面是否已经上过锁**

### 乐观锁的版本号机制

### 乐观锁的时间戳机制

更新提交的时候，将当前数据的时间戳和更新之前取得的时间戳进行比较

悲观锁（Pessimistic Locking）也是一种思想，对数据被其他事务的修改持保守态度，会通过数据库自身的锁机制来实现，从而保证数据操作的排它性。

1. 乐观锁适合读操作多的场景，相对来说写的操作比较少。它的优点在于程序实现，不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。
2. 悲观锁适合写操作多的场景，因为写的操作具有排它性。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止读 - 写和写 - 写的冲突。

## MVCC 是什么，解决了什么问题

多版本并发控制:保存数据的历史版本。这样我们就可以通过比较版本号决定数据是否显示出来

1. 读写之间阻塞的问题，通过 MVCC 可以让读写互相不阻塞，即读不阻塞写，写不阻塞读，这样就可以提升事务并发处理能力。
2. 降低了死锁的概率。这是因为 MVCC 采用了乐观锁的方式，读取数据时并不需要加锁，对于写操作，也只锁定必要的行。
3. 解决一致性读的问题。一致性读也被称为快照读，当我们查询数据库在某个时间点的快照时，只能看到这个时间点之前事务提交更新的结果，而不能看到这个时间点之后事务提交的更新结果。

### 事务版本号

每开启一个事务，我们都会从数据库中获得一个事务 ID（也就是事务版本号），这个事务 ID 是自增长的，通过 ID 大小，我们就可以判断事务的时间顺序。

### 行记录的隐藏列

InnoDB 的叶子段存储了数据页，数据页中保存了行记录，而在行记录中有一些重要的隐藏字段，如下图所示：

1. db_row_id：隐藏的行 ID，用来生成默认聚集索引。如果我们创建数据表的时候没有指定聚集索引，这时 InnoDB 就会用这个隐藏 ID 来创建聚集索引。采用聚集索引的方式可以提升数据的查找效率。
2. db_trx_id：操作这个数据的事务 ID，也就是最后一个对该数据进行插入或更新的事务 ID。
3. db_roll_ptr：回滚指针，也就是指向这个记录的 Undo Log 信息。

### Undo Log

InnoDB 将行记录快照保存在了 Undo Log 里，我们可以在回滚段中找到它们，如下图所示：

在 Read VIew 中有几个重要的属性：

1. trx_ids，系统当前正在活跃的事务 ID 集合。
2. low_limit_id，活跃的事务中最大的事务 ID。
3. up_limit_id，活跃的事务中最小的事务 ID。
4. creator_trx_id，创建这个 Read View 的事务 ID。

## InnoDB 是如何解决幻读的

不过这里需要说明的是，在可重复读的情况下，InnoDB 可以通过 Next-Key 锁 +MVCC 来解决幻读问题。

**介绍下 InnoDB 三种行锁的方式：**

1. 记录锁：针对单个行记录添加锁。
2. 间隙锁（Gap Locking）：可以帮我们锁住一个范围（索引之间的空隙），但不包括记录本身。采用间隙锁的方式可以防止幻读情况的产生。
3. Next-Key 锁：帮我们锁住一个范围，同时锁定记录本身，相当于间隙锁 + 记录锁，可以解决幻读的问题。

## 数据库服务器的优化步骤

![image-20210904113941407](\img\sql6)

## 什么是联合索引的最左原则？

查询“z=7 AND y=8 AND x=9”的时候，如果三个字段 x、y、z 在条件查询的时候是乱序的，但采用的是等值查询（=）或者是 IN 查询，那么 MySQL 的优化器可以自动帮我们调整为可以使用联合索引的形式。

当我们查询“x=9 AND y>8 AND z=7”的时候，如果建立了 (x,y,z) 顺序的索引，这时候 z 是用不上索引的。这是因为 MySQL 在匹配联合索引最左前缀的时候，如果遇到了范围查询，比如（<）（>）和 between 等，就会停止匹配。索引列最多作用于一个范围列，对于后面的 Z 来说，就没法使用到索引了。

## InnoDB 缓冲池

InnoDB 存储引擎中有一部分数据会放到内存中,缓冲池则占了这部分内存的大部分，它用来存储各种数据的缓存，如下图所示：

![image-20210904123606735](\img\sql7)

InnoDB 存储引擎基于磁盘文件存储，访问物理硬盘和在内存中进行访问，速度相差很大，为了尽可能弥补这两者之间 I/O 效率的差值，我们就需要把经常使用的数据加载到缓冲池中，避免每次访问都进行磁盘 I/O。

InnoDB 支持事务和行级锁，是 MySQL 默认的存储引擎；MyISAM 只支持表级锁，不支持事务，更适合读取数据库的情况。

